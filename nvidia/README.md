# NVIDIA Software

### NVIDIA RIVA

NVIDIA Riva is a set of GPU-accelerated multilingual speech and translation microservices for building fully customizable, real-time conversational AI pipelines. Riva includes automatic speech recognition (ASR), [text-to-speech (TTS)](https://www.nvidia.com/en-us/glossary/text-to-speech/?ncid=no-ncid), and neural machine translation (NMT) and is deployable in all clouds, in data centers, at the edge, and on embedded devices. With Riva, organizations can add speech and translation interfaces with [large language models (LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/?ncid=no-ncid) and [retrieval-augmented generation (RAG)](https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/?ncid=no-ncid) to transform chatbots into engaging, expressive multilingual assistants and avatars.

### NeMo Guardrails

NeMo Guardrails is an open-source toolkit for easily adding *programmable guardrails* to LLM-based conversational applications. Guardrails (or "rails" for short) are specific ways of controlling the output of a large language model, such as not talking about politics, responding in a particular way to specific user requests, following a predefined dialog path, using a particular language style, extracting structured data, and more.

### NVIDIA Rapids

Rapids built on top of Apache Arrow and CUDA-x libraries. CUDA-X is built on top of CUDA, a collection of microservices, tools and libraries and technology for building AI and HPC applications that are highly performant and scalable.

- Data Preprocessing → **cuDF** - Accelerate dataframes for efficiently processing hundreds of millions of records.
- Big Data Processing → **RAPIDS Accelerator for Apache Spark -** Accelerate your existing Apache Spark applications with minimal code changes.
- **Machine Learning: cuML** - Execute machine learning algorithms on CPUs and GPUs with an API that closely follows the scikit-learn API.
- **Graph Analytics: cuGraph** - Quickly navigate graph analytics libraries with a python API that follows NetworkX.
- **Vector Search: cuVS** - Apply cuVS algorithms to accelerate vector search, including world-class performance from CAGRA.
- **Scale RAPIDS: Dask-RAPIDS** - Expand data science pipelines to multiple nodes with RAPIDS on Dask.
- **Visualization: cu-x-filter** - Create interactive data visuals with multidimensional filtering of over 100-million-row tabular datasets.
- **Image: cuCIM** - Accelerate input/output (IO), computer vision, and image processing of n-dimensional, especially biomedical, images.
- **Neural Networks: cuDNN** - The [**NVIDIA CUDA**](https://developer.nvidia.com/cudnn)® [**Deep Neural Network (cuDNN)**](https://developer.nvidia.com/cudnn) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.

### NVIDIA NeMo

NVIDIA NeMo™ is an end-to-end platform for developing custom generative AI—including large language models (LLMs), vision language models (VLMs), video models, and [speech AI](https://www.nvidia.com/en-us/ai-data-science/solutions/speech-ai/?ncid=no-ncid)—anywhere.

Deliver enterprise-ready models with precise data curation, cutting-edge customization, [retrieval-augmented generation](https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/?ncid=no-ncid) (RAG), and accelerated performance with NeMo, part of [NVIDIA AI Foundry](https://www.nvidia.com/en-us/ai/foundry/?ncid=no-ncid)—a platform and service for building custom generative AI models with enterprise data and domain-specific knowledge.

**Nemo Megatron** - large scale transformer models - model parallelism across multiple GPU multiple Nodes

### NIM

NVIDIA NIM™ provides prebuilt, optimized inference microservices that let you deploy the latest AI foundation models with security and stability on any NVIDIA-accelerated infrastructure— cloud, data center, and workstation.

### NVIDIA DGX

NVIDIA DGX™ Cloud is a high-performance, fully managed AI platform that provides accelerated computing clusters on any leading cloud with flexible term lengths and access to NVIDIA experts to fast-track AI initiatives.

### NVIDIA Triton Inference Server

Run inference on trained machine learning or deep learning models from any framework on any processor—GPU, CPU, or other—with NVIDIA Triton™ Inference Server. Part of the NVIDIA AI platform and available with NVIDIA AI Enterprise, Triton Inference Server is open-source software that standardizes AI model deployment and execution across every workload.

### AI enterprise

NVIDIA AI Enterprise is a cloud-native software platform that streamlines development and deployment of production-grade, end-to-end generative AI pipelines and helps organizations build data flywheels for the next era of agentic AI.

Deploy anywhere—cloud, data center, edge, or workstation. Easy-to-use microservices optimize model performance with enterprise-grade security, support, and stability, ensuring a smooth transition from prototype to production.

### TensorRT

NVIDIA® TensorRT™ is an ecosystem of APIs for high-performance deep learning inference. TensorRT includes an **inference** runtime and model optimizations that deliver low latency and high throughput for production applications. The TensorRT ecosystem includes TensorRT, TensorRT-LLM, TensorRT Model Optimizer, and TensorRT Cloud.

### OMNIVERSE

NVIDIA Omniverse™ is a platform of APIs, SDKs, and services that enable developers to integrate [OpenUSD](https://www.nvidia.com/en-us/omniverse/usd/), NVIDIA RTX™ rendering technologies, and generative [physical AI](https://www.nvidia.com/en-us/glossary/generative-physical-ai/) into existing software tools and simulation workflows for industrial and robotic use cases.

### NGC Catalog

GPU Accelerated AI models and SDKs that help you infuse AI into your applications at speed of light

### Nvidia JARVIS

***NVIDIA Jarvis*** is an application framework for multimodal conversational AI services that delivers real-time performance on GPUs.

### Nvidia Merlin

[NVIDIA Merlin](https://github.com/NVIDIA-Merlin/Merlin) empowers data scientists, machine learning engineers, and researchers to build high-performing recommenders at scale. Merlin includes libraries, methods, and tools that streamline the building of recommenders by addressing common preprocessing, feature engineering, training, inference, and deploying to production challenges.

### Nvidia Clara

NVIDIA Clara™ is a suite of computing platforms, software, and services that powers AI solutions for healthcare and life sciences, from imaging and instruments to genomics and drug discovery.

### Nvidia Morpheus

[NVIDIA Morpheus](https://www.nvidia.com/en-us/ai-data-science/products/morpheus/) is a GPU-accelerated, end-to-end AI framework that enables developers to create optimized applications for filtering, processing, and classifying large volumes of streaming cybersecurity data. Morpheus incorporates AI to reduce the time and cost associated with identifying, capturing, and acting on threats, bringing a new level of security to the data center, cloud, and edge. Morpheus also extends human analysts’ capabilities with generative AI by automating real-time analysis and responses, producing synthetic data to train AI models that identify risks accurately, and run what-if scenarios

### Nvidia Maxine

**NVIDIA Maxine™** is a collection of high-performance, easy-to-use, NVIDIA NIM™ microservices and SDKs for deploying AI features that enhance audio, video, and augmented reality (AR) effects for video conferencing and telepresence.

### Nvidia Jetson

Nvidia Jetson is a series of embedded computing boards from Nvidia. The Jetson TK1, TX1 and TX2 models all carry a Tegra processor (or SoC) from Nvidia that integrates an ARM architecture central processing unit (CPU). Jetson is a low-power system and is designed for accelerating machine learning applications.

### Nvidia Metropolis

NVIDIA Metropolis is an application framework, set of developer tools, and ecosystem of companies that brings visual data and AI together to improve operational efficiency and safety across a variety of industries. It helps make sense of data created by trillions of sensors for some of the world’s most valuable physical transactions. These include frictionless retail, streamlined inventory management, traffic engineering in smart cities, optical inspection on factory floors, patient care in healthcare facilities, and more. Take advantage of the Metropolis developer tools and ecosystem to create, deploy, and scale AI and IoT applications from the edge to the cloud.

### Nvidia Deepstream

NVIDIA’s DeepStream SDK is a complete streaming analytics toolkit based on GStreamer for AI-based multi-sensor processing, video, audio, and image understanding. It’s ideal for vision AI developers, software partners, startups, and OEMs building IVA apps and services.

You can now create stream-processing pipelines that incorporate neural networks and other complex processing tasks like tracking, video encoding/decoding, and video rendering. These pipelines enable real-time analytics on video, image, and sensor data.

### **NCCL**

Optimized primitives for inter-GPU communication.

NCCL (pronounced "Nickel") is a stand-alone library of standard communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, as well as any send/receive based communication pattern. It has been optimized to achieve high bandwidth on platforms using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets. NCCL supports an arbitrary number of GPUs installed in a single node or across multiple nodes, and can be used in either single- or multi-process (e.g., MPI) applications.

### NVLINK

NVIDIA® NVLink™ is the world's first high-speed GPU interconnect offering a significantly faster alternative for multi-GPU systems than traditional PCIe-based solutions. Connecting two NVIDIA® graphics cards with NVLink enables scaling of memory and performance1 to meet the demands of your largest visual computing workloads.

### DALI

DALI is a high-performance alternative to built-in data loaders and data iterators. You can now run your data processing pipelines on the GPU, reducing the total time it takes to train a neural network. Data processing pipelines implemented using DALI are portable because they can easily be retargeted to TensorFlow, PyTorch, and MXNet.

### Nsight system

NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest systems-on-a-chip (SoCs).